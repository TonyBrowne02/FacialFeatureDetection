{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating template\n",
    "import cv2\n",
    "# 350 100 \n",
    "# 420 150\n",
    "# Load a face image\n",
    "face_img = cv2.imread('../../Images/Trump.jpg')\n",
    "\n",
    "# Manually crop the eye region (specify the coordinates)\n",
    "# Coordinates are (x, y, width, height) for the cropping area\n",
    "# Coordinates: (y1:y2, x1:x2) -> (top:bottom, left:right)\n",
    "eye_template = face_img[100:150, 350:420]\n",
    "\n",
    "# Save the cropped eye template\n",
    "cv2.imwrite('eye_template.jpg', eye_template)\n",
    "\n",
    "# Display the eye template\n",
    "cv2.imshow('Eye Template', eye_template)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE MATCHING\n",
    "\n",
    "# A small image (template) of the feature is created, and the algorithm scans through the larger image to \n",
    "# find areas that resemble the template.\n",
    "# It computes a similarity score (e.g., using normalized cross-correlation) between the template \n",
    "# and various subregions of the image.\n",
    "# The region with the highest score is considered the location of the feature.\n",
    "\n",
    "#Use Cases: Detecting specific features like eyes, mouth, or nose from a given template.\n",
    "import cv2\n",
    "\n",
    "# Load the main image and the template (e.g., an eye image)\n",
    "img = cv2.imread(\"../../Images/Zardoz.jpg\", 0)\n",
    "\n",
    "template = cv2.imread('eye_template.jpg', 0)\n",
    "\n",
    "# Apply template matching\n",
    "result = cv2.matchTemplate(img, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Find the location of the best match\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "# Draw rectangle around the detected feature\n",
    "h, w = template.shape[:2]\n",
    "cv2.rectangle(img, max_loc, (max_loc[0] + w, max_loc[1] + h), 255, 2)\n",
    "\n",
    "cv2.imshow('Detected', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edge Detection + Geometrical Shape Detection\n",
    "\n",
    "# First, use edge detection to highlight boundaries in the image.\n",
    "# Then, detect circular or elliptical shapes that match the general outline of facial features \n",
    "# (like the eye socket or iris).\n",
    "#Use Cases: Detecting circular features like irises or the contour of lips and eyes.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load image and convert to grayscale\n",
    "img = cv2.imread('../../Images/Zardoz.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply edge detection\n",
    "edges = cv2.Canny(gray, 100, 200)\n",
    "\n",
    "# Detect circles using Hough Transform\n",
    "circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=100, param1=100, param2=30, minRadius=10, maxRadius=80)\n",
    "\n",
    "# Draw circles if detected\n",
    "if circles is not None:\n",
    "    circles = np.round(circles[0, :]).astype(\"int\")\n",
    "    for (x, y, r) in circles:\n",
    "        cv2.circle(img, (x, y), r, (0, 255, 0), 2)\n",
    "\n",
    "# Display result\n",
    "cv2.imshow('Detected Circles', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color Segmentation (Thresholding)\n",
    "\n",
    "# Convert the image to a different color space (e.g., HSV) and apply thresholding to \n",
    "# isolate facial featuresbased on their color.\n",
    "# You can use either binary thresholding or adaptive thresholding depending on the feature being detected.\n",
    "\n",
    "# Use Cases: Detecting lips, eyes (based on darkness), \n",
    "# or any feature that differs in color from the surrounding areas.\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the image and convert to HSV color space\n",
    "img = cv2.imread('../../Images/Zardoz.jpg')\n",
    "hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define lower and upper bounds for lip color (adjust based on image)\n",
    "lower_bound = np.array([150, 50, 50])  # Example HSV lower bound for lip color\n",
    "upper_bound = np.array([180, 255, 255])  # Example HSV upper bound for lip color\n",
    "\n",
    "# Apply the color mask\n",
    "mask = cv2.inRange(hsv_img, lower_bound, upper_bound)\n",
    "\n",
    "# Apply mask to the image\n",
    "result = cv2.bitwise_and(img, img, mask=mask)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Lips Detected', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometrical Feature Detection (Distance Ratios and Proportions)\n",
    "\n",
    "# After detecting the face, you can apply geometric rules to estimate the location of eyes, nose, \n",
    "# and mouth based on the relative positions of known points \n",
    "# (e.g., corners of the mouth are usually positioned below and aligned with the eyes).\n",
    "# This requires detecting basic geometrical shapes (rectangles, circles) or edges that correspond to facial regions.\n",
    "import cv2\n",
    "\n",
    "# Load the Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load the image and convert it to grayscale\n",
    "img = cv2.imread(\"../../Images/Zardoz.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect faces\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "# For each detected face, apply the geometrical feature detection\n",
    "for (x, y, w, h) in faces:\n",
    "    # Draw rectangle around detected face\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    # Estimate feature locations using geometrical proportions\n",
    "    # 1. Eyes: Around 1/3 down the face from the top\n",
    "    eye_y = int(y + h * 0.3)\n",
    "    eye_left_x = int(x + w * 0.25)  # Left eye, 1/4th across the width of the face\n",
    "    eye_right_x = int(x + w * 0.75)  # Right eye, 3/4th across the width\n",
    "\n",
    "    # 2. Nose: Around halfway down the face\n",
    "    nose_y = int(y + h * 0.5)\n",
    "    nose_x = int(x + w * 0.5)  # Center of the face for the nose\n",
    "\n",
    "    # 3. Mouth: Around 2/3 down the face\n",
    "    mouth_y = int(y + h * 0.7)\n",
    "    mouth_left_x = int(x + w * 0.3)  # Left corner of the mouth\n",
    "    mouth_right_x = int(x + w * 0.7)  # Right corner of the mouth\n",
    "\n",
    "    # Draw circles to mark the estimated positions of the eyes, nose, and mouth\n",
    "    cv2.circle(img, (eye_left_x, eye_y), 5, (0, 255, 0), -1)  # Left eye\n",
    "    cv2.circle(img, (eye_right_x, eye_y), 5, (0, 255, 0), -1)  # Right eye\n",
    "    cv2.circle(img, (nose_x, nose_y), 5, (0, 0, 255), -1)  # Nose\n",
    "    cv2.circle(img, (mouth_left_x, mouth_y), 5, (255, 0, 255), -1)  # Left corner of the mouth\n",
    "    cv2.circle(img, (mouth_right_x, mouth_y), 5, (255, 0, 255), -1)  # Right corner of the mouth\n",
    "\n",
    "# Display the output image with estimated facial features\n",
    "cv2.imshow('Geometrical Feature Detection', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Oriented Gradients (HOG) for Feature Detection\n",
    "# %pip install scikit-image\n",
    "\n",
    "# Compute the gradient of the image in the X and Y directions.\n",
    "# Divide the image into small cells, compute histograms of the gradient orientations in each cell, and normalize them.\n",
    "# The resulting descriptor can help find structures like eyes and lips, \n",
    "# which typically exhibit distinct gradient patterns.\n",
    "# Use Cases: Detecting local patterns corresponding to facial features.\n",
    "from skimage.feature import hog\n",
    "from skimage import io, color\n",
    "\n",
    "# Load and convert image to grayscale\n",
    "img = io.imread(\"Images\\\\Trump.jpg\")\n",
    "gray_img = color.rgb2gray(img)\n",
    "\n",
    "# Compute HOG descriptor\n",
    "hog_features, hog_image = hog(gray_img, visualize=True, block_norm='L2-Hys', pixels_per_cell=(16, 16))\n",
    "\n",
    "# Display the result\n",
    "io.imshow(hog_image)\n",
    "io.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Shape Models (ASM) or Active Appearance Models (AAM)\n",
    "\n",
    "# An initial model of the face (e.g., contours of eyes, nose, and mouth) is iteratively adjusted to \n",
    "# fit the facial features in a new image based on shape and texture information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
