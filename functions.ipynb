{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#Reizing image\n",
    "def resize(img, height=640):\n",
    "    img_X_scale = height / img.shape[0]\n",
    "    new_img_width = int(img.shape[1] * img_X_scale)\n",
    "    img_resized = cv2.resize(img, (int(new_img_width), int(height)))\n",
    "    return img_resized\n",
    "\n",
    "# img = cv2.imread(\"./FacialImages/person0.jpg\")\n",
    "# cv2.imshow(\"original image\", img)\n",
    "# cv2.imshow(\"resized\",resize(img))\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise Reduction using Bi-Lateral Filter as edge preservation is critical and runtime is a non factor\n",
    "\n",
    "# Can create \"painting-like\" effects if parameters are too strong\n",
    "# Diameter (d): Size of pixel neighborhood\n",
    "# sigmaColor: How much differences in color/intensity are tolerated\n",
    "# sigmaSpace: How much spatial distance is tolerated\n",
    "# Start with d=9, sigmaColor=75, sigmaSpace=75\n",
    "# Increase sigmaColor if noise persists\n",
    "# Decrease sigmaColor if edges become too blurry\n",
    "# Adjust d based on image resolution (larger for higher-res images)\n",
    "\n",
    "def BiLateralFitlerDenoise(img, diameter = 9, sigma_color=75, sigma_space=75 ):\n",
    "    return cv2.bilateralFilter(img, diameter, sigma_color, sigma_space)\n",
    "\n",
    "\n",
    "# img = cv2.imread(\"./FacialImages/person0.jpg\")\n",
    "# img = resize(img)\n",
    "\n",
    "# cv2.imshow(\"original image\", resize(img))\n",
    "# cv2.imshow(\"Denoised\",BiLateralFitlerDenoise(img))\n",
    "\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_grayscale(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return True\n",
    "    if image.shape[2] == 1:#no of colour channels \n",
    "        return True\n",
    "    \n",
    "    channels = cv2.split(image)\n",
    "    return np.allclose(channels[0], channels[1]) and np.allclose(channels[0], channels[2]) # checks if the values are close "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance Contrast CLAHE\n",
    "\n",
    "# clipLimit: Controls contrast enhancement\n",
    "    # Higher values = more contrast but more noise\n",
    "    # Lower values = less contrast but less noise\n",
    "    # Typical values: 2.0 - 4.0\n",
    "\n",
    "# tileGridSize: Size of local regions\n",
    "    # Smaller tiles = more local enhancement\n",
    "    # Larger tiles = more global enhancement\n",
    "    # Typical values: (8,8) or (16,16)\n",
    "def enhanceContrast(img):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    if (is_grayscale(img)):#check if gray\n",
    "        \n",
    "        enhanced = clahe.apply(img)\n",
    "        return enhanced\n",
    "    else:\n",
    "        # Convert to LAB color space\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Apply CLAHE to L channel\n",
    "        enhanced_l = clahe.apply(l)\n",
    "        \n",
    "        # Merge and convert back\n",
    "        enhanced_lab = cv2.merge([enhanced_l, a, b])\n",
    "        return cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "img = cv2.imread(\"./FacialImages/person0.jpg\")\n",
    "img = resize(img)\n",
    "\n",
    "cv2.imshow(\"original image\", resize(img))\n",
    "cv2.imshow(\"Contrast Enhanced\", enhanceContrast(img))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minmax normalising \n",
    "#TODO Maybe different normalising, min maxing a gray image from 0 to 255 does nothing\n",
    "def normalise(img):\n",
    "        new_min = 0\n",
    "        new_max = 255\n",
    "        old_min, old_max = img.min(), img.max()\n",
    "        normalized = (img - old_min) / (old_max - old_min)\n",
    "        normalized = normalized * (new_max - new_min) + new_min\n",
    "        return normalized.astype(img.dtype)\n",
    "\n",
    "img = cv2.imread(\"./FacialImages/person0.jpg\")\n",
    "img = resize(img)\n",
    "\n",
    "\n",
    "#Showing hist\n",
    "normalised = normalise(img)\n",
    "\n",
    "\n",
    "cv2.imshow(\"original image\", img)\n",
    "cv2.imshow(\"Normalised\", normalised)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unsharp masking\n",
    "    # kernel_size (tuple): The size of the Gaussian blur kernel.\n",
    "    # sigma (float): The standard deviation of the Gaussian blur kernel.\n",
    "    # amount (float): The amount of sharpening to apply.\n",
    "    # threshold (int): The minimum brightness change that will be sharpened.\n",
    "\n",
    "def unsharpMask(img, kernel_size=5, sigma=1.5, amount =1.0, threshold=0):\n",
    "    blurred = cv2.GaussianBlur(img, (kernel_size, kernel_size), sigma, sigma)\n",
    "\n",
    "    sharpened = cv2.addWeighted(img, 1 + amount, blurred, -amount, 0)\n",
    "\n",
    "    # Apply a threshold to the sharpening\n",
    "    image_sharp = np.where((img - blurred) < -threshold, 0, sharpened)\n",
    "\n",
    "    return image_sharp\n",
    "\n",
    "cv2.imshow(\"original image\", resize(img))\n",
    "cv2.imshow(\"UnSharpMasked\", unsharpMask(resize(img)))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation: -14.182644\n"
     ]
    }
   ],
   "source": [
    "#vertical alignment\n",
    "# takes in cropped region of face?\n",
    "# not sure if this will be necessary\n",
    "import cv2\n",
    "def verticalAlign(img):\n",
    "        # Rotate the image, white mask, and green mask\n",
    "        grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        edge = cv2.Canny(grey, 200, 255, None, 3)\n",
    "        \n",
    "        lines = cv2.HoughLines(edge, 1, np.pi / 180, 200)\n",
    "        if lines is None:\n",
    "            print(\"lines is Non\")\n",
    "            return img\n",
    "        \n",
    "        angles = []\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            angle = np.degrees(theta) - 90\n",
    "            if -45 < angle < 45:\n",
    "                angles.append(angle)\n",
    "\n",
    "        if len(angles) == 0:\n",
    "            print(\"len angles 0\")\n",
    "            return img\n",
    "        \n",
    "        rotation = np.mean(angles)\n",
    "        print(\"Rotation:\", rotation)\n",
    "        (h, w) = edge.shape[:2]\n",
    "\n",
    "        center = (w // 2, h // 2)\n",
    "        rotateMatrix = cv2.getRotationMatrix2D(center, rotation, 1.0)\n",
    "        \n",
    "        rotated_img = cv2.warpAffine(img, rotateMatrix, (w, h))\n",
    "    \n",
    "    \n",
    "        return rotated_img\n",
    "\n",
    "img = cv2.imread(\"./FacialImages/DJCroppedFace.jpeg\")\n",
    "img = resize(img)\n",
    "img = enhanceContrast(img)\n",
    "align = verticalAlign(img)\n",
    "cv2.imshow(\"original image\", img)\n",
    "cv2.imshow(\"Aligned\", align)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
